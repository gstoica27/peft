{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "bdb26eba-cbdf-4d59-b6e2-57e8e69d5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    ")\n",
    "\n",
    "import pdb\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import PeftModel, PeftConfig, load_peft_weights, set_peft_model_state_dict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "2d4e0af7-d4eb-4629-adc2-f06d92d9a58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(combinations(tasks,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "9a8986ba-a047-4d52-8aab-9ab22ae459b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    #'cola',\n",
    "    'sst2',\n",
    "    'mrpc',\n",
    "    # 'mnli', # This doesn't have a validation set.... (don't have)\n",
    "    # 'qnli', # We don't have enough memory to train this.. ((don't have))\n",
    "    'rte',\n",
    "    #'wnli',\n",
    "    #'stsb', # This data isn't formatted how the next code block expects it to be (don't have)\n",
    "    'qqp', # This takes 1hr per epoch...\n",
    "]\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658cbd06-538c-42c0-a7fe-29d3df6d9832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "63e0ab43-bba4-4b29-a751-883adc4d1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sst2', 'mrpc', 'rte', 'qqp')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001dfc15561a4f92a155ebffe1144a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/390965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                       | 0/28 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:01<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst2 {'accuracy': 0.5091743119266054}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrpc {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:02<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rte {'accuracy': 0.4729241877256318}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1264/1264 [01:59<00:00, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qqp {'accuracy': 0.3681919366806827, 'f1': 0.53820009400875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_evals = []\n",
    "all_metrics = []\n",
    "all_model_ids = []\n",
    "zero_shot = True\n",
    "\n",
    "batch_size = 32\n",
    "model_name_or_path = \"roberta-large\"\n",
    "peft_type = PeftType.LORA\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "\n",
    "\n",
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "    \n",
    "all_tasks = list(combinations(tasks,4))\n",
    "for tasks in all_tasks:\n",
    "    print(tasks)\n",
    "    for task in tasks:\n",
    "        datasets = load_dataset(\"glue\", task, cache_dir='/srv/hoffman-lab/flash9/pramesh39/datasets/GLUE')\n",
    "        metric = evaluate.load(\"glue\", task, cache_dir='/srv/hoffman-lab/flash9/pramesh39/datasets/GLUE')\n",
    "        sentence1_key, sentence2_key = task_to_keys[task]\n",
    "    \n",
    "        def tokenize_function(examples):\n",
    "            # max_length=None => use the model max length (it's actually the default)\n",
    "            args = (\n",
    "                (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "            )\n",
    "            outputs = tokenizer(*args, truncation=True, max_length=None)\n",
    "            return outputs\n",
    "    \n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"idx\"] + list([t for t in task_to_keys[task] if t is not None]),\n",
    "        )\n",
    "        tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "        \n",
    "        train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
    "        eval_dataloader = DataLoader(\n",
    "            tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size\n",
    "        )\n",
    "        all_evals.append(eval_dataloader)\n",
    "        all_metrics.append(metric)\n",
    "        all_model_ids.append(\"gstoica3/roberta-large-peft-\" + task)\n",
    "    \n",
    "    #Merged the models\n",
    "    merged_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "    if(zero_shot == False):\n",
    "        merged_model = PeftModel.from_pretrained(merged_model, all_model_ids[0])\n",
    "        adapters_weights = load_peft_weights(all_model_ids[0], device=device)\n",
    "        \n",
    "        for all_model_id in all_model_ids[1:]:\n",
    "            new_adapter = load_peft_weights(all_model_id, device=device)\n",
    "            for key in new_adapter:\n",
    "                adapters_weights[key] += new_adapter[key]\n",
    "        \n",
    "        load_result = set_peft_model_state_dict(merged_model, adapters_weights) \n",
    "    \n",
    "    #Evaluate the merged model\n",
    "    merged_model.to(device)\n",
    "    merged_model.eval()\n",
    "    for task_id, task in enumerate(tasks):\n",
    "        for step, batch in enumerate(tqdm(all_evals[task_id])):\n",
    "            batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = merged_model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            predictions, references = predictions, batch[\"labels\"]\n",
    "            all_metrics[task_id].add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "        \n",
    "        eval_metric = all_metrics[task_id].compute()\n",
    "        print(task, eval_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "37b5d0d5-a0bc-4049-b898-6e99a2463dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                       | 0/28 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:02<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst2 {'accuracy': 0.9575688073394495}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1264/1264 [02:06<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qqp {'accuracy': 0.9147167944595598, 'f1': 0.8871136720796228}\n"
     ]
    }
   ],
   "source": [
    "# for task_id, task in enumerate(tasks):\n",
    "#     config = PeftConfig.from_pretrained(all_model_ids[task_id])\n",
    "#     inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "#     #tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "    \n",
    "#     # Load the Lora model\n",
    "#     inference_model = PeftModel.from_pretrained(inference_model, all_model_ids[task_id])\n",
    "    \n",
    "#     inference_model.to(device)\n",
    "#     inference_model.eval()\n",
    "#     for step, batch in enumerate(tqdm(all_evals[task_id])):\n",
    "#         batch.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = inference_model(**batch)\n",
    "#         predictions = outputs.logits.argmax(dim=-1)\n",
    "#         predictions, references = predictions, batch[\"labels\"]\n",
    "#         all_metrics[task_id].add_batch(\n",
    "#             predictions=predictions,\n",
    "#             references=references,\n",
    "#         )\n",
    "    \n",
    "#     eval_metric = all_metrics[task_id].compute()\n",
    "#     print(task, eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7c8152a3-bd31-444d-b9a0-b4ad277e4cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# merged_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "# # # tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "# merged_model = PeftModel.from_pretrained(merged_model, all_model_ids[0])\n",
    "# adapters_weights = load_peft_weights(all_model_ids[0], device=device)\n",
    "\n",
    "# for all_model_id in all_model_ids[1:]:\n",
    "#     new_adapter = load_peft_weights(all_model_id, device=device)\n",
    "#     for key in new_adapter:\n",
    "#         adapters_weights[key] += new_adapter[key]\n",
    "\n",
    "# load_result = set_peft_model_state_dict(merged_model, adapters_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "5bea28fa-abbe-4de4-8603-b206d5d0c40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:02<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst2 {'accuracy': 0.6376146788990825}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1264/1264 [02:07<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qqp {'accuracy': 0.7787039327232254, 'f1': 0.7173590270099511}\n"
     ]
    }
   ],
   "source": [
    "# merged_model.to(device)\n",
    "# merged_model.eval()\n",
    "# for task_id, task in enumerate(tasks):\n",
    "#     for step, batch in enumerate(tqdm(all_evals[task_id])):\n",
    "#         batch.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = merged_model(**batch)\n",
    "#         predictions = outputs.logits.argmax(dim=-1)\n",
    "#         predictions, references = predictions, batch[\"labels\"]\n",
    "#         all_metrics[task_id].add_batch(\n",
    "#             predictions=predictions,\n",
    "#             references=references,\n",
    "#         )\n",
    "    \n",
    "#     eval_metric = all_metrics[task_id].compute()\n",
    "#     print(task, eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a8278-7ddb-4b8d-9f33-cab547a8da45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "36996c7d-2958-4822-8e1e-e0e2296c5d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['base_model.model.base_model.model.roberta.embeddings.word_embeddings.weight', 'base_model.model.base_model.model.roberta.embeddings.position_embeddings.weight', 'base_model.model.base_model.model.roberta.embeddings.token_type_embeddings.weight', 'base_model.model.base_model.model.roberta.embeddings.LayerNorm.weight', 'base_model.model.base_model.model.roberta.embeddings.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.0.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.0.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.1.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.1.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.2.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.2.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.3.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.3.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.4.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.4.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.5.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.5.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.6.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.6.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.7.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.7.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.8.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.8.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.9.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.9.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.10.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.10.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.11.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.11.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.12.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.12.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.12.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.12.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.13.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.13.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.13.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.13.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.14.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.14.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.14.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.14.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.15.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.15.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.15.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.15.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.16.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.16.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.16.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.16.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.17.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.17.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.17.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.17.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.18.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.18.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.18.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.18.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.19.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.19.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.19.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.19.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.20.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.20.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.20.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.20.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.21.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.21.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.21.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.21.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.22.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.22.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.22.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.22.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.query.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.query.bias', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.key.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.key.bias', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.value.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.value.bias', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.model.base_model.model.roberta.encoder.layer.23.intermediate.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.intermediate.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.23.output.dense.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.output.dense.bias', 'base_model.model.base_model.model.roberta.encoder.layer.23.output.LayerNorm.weight', 'base_model.model.base_model.model.roberta.encoder.layer.23.output.LayerNorm.bias', 'base_model.model.base_model.model.classifier.original_module.dense.weight', 'base_model.model.base_model.model.classifier.original_module.dense.bias', 'base_model.model.base_model.model.classifier.original_module.out_proj.weight', 'base_model.model.base_model.model.classifier.original_module.out_proj.bias', 'base_model.model.base_model.model.classifier.modules_to_save.default.dense.weight', 'base_model.model.base_model.model.classifier.modules_to_save.default.dense.bias', 'base_model.model.base_model.model.classifier.modules_to_save.default.out_proj.weight', 'base_model.model.base_model.model.classifier.modules_to_save.default.out_proj.bias'])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9d20cf37-0bd8-4746-a3f5-5f9d37bc3ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "424dda50-8cdf-4826-b8f7-113e8f76caa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0324,  0.0208,  0.0292,  ..., -0.0181,  0.0363, -0.0287],\n",
       "        [-0.0386, -0.0438, -0.0166,  ...,  0.0963,  0.0596, -0.0434],\n",
       "        [ 0.0043,  0.0122, -0.0232,  ...,  0.0030,  0.0238,  0.0308],\n",
       "        ...,\n",
       "        [ 0.0127, -0.0332, -0.0553,  ...,  0.0156, -0.0294, -0.0243],\n",
       "        [-0.0258, -0.0459,  0.0265,  ..., -0.0770,  0.0154,  0.0336],\n",
       "        [-0.0325,  0.0043, -0.0517,  ..., -0.0407,  0.0424,  0.0241]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapters_weights['base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a886d2c6-eb00-453d-a596-9d6551bf9be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0324,  0.0208,  0.0292,  ..., -0.0181,  0.0363, -0.0287],\n",
       "        [-0.0386, -0.0438, -0.0166,  ...,  0.0963,  0.0596, -0.0434],\n",
       "        [ 0.0043,  0.0122, -0.0232,  ...,  0.0030,  0.0238,  0.0308],\n",
       "        ...,\n",
       "        [ 0.0127, -0.0332, -0.0553,  ...,  0.0156, -0.0294, -0.0243],\n",
       "        [-0.0258, -0.0459,  0.0265,  ..., -0.0770,  0.0154,  0.0336],\n",
       "        [-0.0325,  0.0043, -0.0517,  ..., -0.0407,  0.0424,  0.0241]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = merged_model.state_dict()['base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight'].to(device)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "432f1394-bc6a-4bdb-8336-eb98868ee748",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters_weights0 = load_peft_weights(all_model_ids[0], device=device)\n",
    "adapters_weights1 = load_peft_weights(all_model_ids[1], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2161743b-e189-4eee-bbc0-e4a23f392e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136,  0.0236, -0.0172,  ..., -0.0050,  0.0016,  0.0076],\n",
       "        [-0.0288, -0.0110, -0.0097,  ...,  0.0592,  0.0269, -0.0405],\n",
       "        [ 0.0067, -0.0096, -0.0102,  ...,  0.0206, -0.0029,  0.0235],\n",
       "        ...,\n",
       "        [ 0.0413, -0.0334, -0.0304,  ..., -0.0022, -0.0205, -0.0108],\n",
       "        [-0.0164, -0.0117,  0.0170,  ..., -0.0249, -0.0279,  0.0334],\n",
       "        [-0.0144,  0.0010, -0.0440,  ..., -0.0247,  0.0348,  0.0029]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapters_weights0['base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4479f9c2-1d59-4cf8-b7c3-475d80df83dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0188, -0.0028,  0.0464,  ..., -0.0131,  0.0347, -0.0363],\n",
       "        [-0.0098, -0.0328, -0.0069,  ...,  0.0370,  0.0326, -0.0029],\n",
       "        [-0.0024,  0.0218, -0.0131,  ..., -0.0176,  0.0267,  0.0073],\n",
       "        ...,\n",
       "        [-0.0286,  0.0002, -0.0248,  ...,  0.0179, -0.0089, -0.0135],\n",
       "        [-0.0093, -0.0342,  0.0095,  ..., -0.0521,  0.0433,  0.0001],\n",
       "        [-0.0180,  0.0032, -0.0077,  ..., -0.0160,  0.0077,  0.0212]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapters_weights1['base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b9b3ca2f-bb9a-4ae9-9bdb-292cbd0bbe18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0324,  0.0208,  0.0292,  ..., -0.0181,  0.0363, -0.0287],\n",
       "        [-0.0386, -0.0438, -0.0166,  ...,  0.0963,  0.0596, -0.0434],\n",
       "        [ 0.0043,  0.0122, -0.0232,  ...,  0.0030,  0.0238,  0.0308],\n",
       "        ...,\n",
       "        [ 0.0127, -0.0332, -0.0553,  ...,  0.0156, -0.0294, -0.0243],\n",
       "        [-0.0258, -0.0459,  0.0265,  ..., -0.0770,  0.0154,  0.0336],\n",
       "        [-0.0325,  0.0043, -0.0517,  ..., -0.0407,  0.0424,  0.0241]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = adapters_weights0['base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.weight'] + adapters_weights1['base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.weight']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9fa25cfd-8871-4302-8a39-e3ad8cce8345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]], device='cuda:0')"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "16f7d73a-f665-4b76-88c7-093366d1659a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash9/pramesh39/miniconda3/envs/peft/lib/python3.10/site-packages/peft/utils/save_and_load.py:102\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name)\u001b[0m\n\u001b[1;32m    100\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules_to_save\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpeft_model_state_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(module_name \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01mfor\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodules_to_save):\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodules_to_save:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "load_result = set_peft_model_state_dict(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "998f96a5-f98d-4077-8a1e-7869acaa670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adapters_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "818a938c-9de0-49f6-999e-73e33f94151a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136,  0.0236, -0.0172,  ..., -0.0050,  0.0016,  0.0076],\n",
       "        [-0.0288, -0.0110, -0.0097,  ...,  0.0592,  0.0269, -0.0405],\n",
       "        [ 0.0067, -0.0096, -0.0102,  ...,  0.0206, -0.0029,  0.0235],\n",
       "        ...,\n",
       "        [ 0.0413, -0.0334, -0.0304,  ..., -0.0022, -0.0205, -0.0108],\n",
       "        [-0.0164, -0.0117,  0.0170,  ..., -0.0249, -0.0279,  0.0334],\n",
       "        [-0.0144,  0.0010, -0.0440,  ..., -0.0247,  0.0348,  0.0029]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "af32bf5a-884a-4247-bde0-36907ef3dd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136,  0.0236, -0.0172,  ..., -0.0050,  0.0016,  0.0076],\n",
       "        [-0.0288, -0.0110, -0.0097,  ...,  0.0592,  0.0269, -0.0405],\n",
       "        [ 0.0067, -0.0096, -0.0102,  ...,  0.0206, -0.0029,  0.0235],\n",
       "        ...,\n",
       "        [ 0.0413, -0.0334, -0.0304,  ..., -0.0022, -0.0205, -0.0108],\n",
       "        [-0.0164, -0.0117,  0.0170,  ..., -0.0249, -0.0279,  0.0334],\n",
       "        [-0.0144,  0.0010, -0.0440,  ..., -0.0247,  0.0348,  0.0029]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapters_weights0['base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d92516a-3d01-445d-b7b0-40f95c960a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf5f8bf4-100f-4c19-ae25-22e5cc218dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='roberta-large', revision=None, task_type='SEQ_CLS', inference_mode=True, r=8, target_modules=['value', 'query'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "986b904c-b565-4c71-aa66-f327d6af78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_result = set_peft_model_state_dict(inference_model, adapters_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6edf77f4-25e4-4f30-b230-535f5c8a0012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136,  0.0236, -0.0172,  ..., -0.0050,  0.0016,  0.0076],\n",
       "        [-0.0288, -0.0110, -0.0097,  ...,  0.0592,  0.0269, -0.0405],\n",
       "        [ 0.0067, -0.0096, -0.0102,  ...,  0.0206, -0.0029,  0.0235],\n",
       "        ...,\n",
       "        [ 0.0413, -0.0334, -0.0304,  ..., -0.0022, -0.0205, -0.0108],\n",
       "        [-0.0164, -0.0117,  0.0170,  ..., -0.0249, -0.0279,  0.0334],\n",
       "        [-0.0144,  0.0010, -0.0440,  ..., -0.0247,  0.0348,  0.0029]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model.state_dict()['base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edf255-d52d-40eb-bbe6-c969541185b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
